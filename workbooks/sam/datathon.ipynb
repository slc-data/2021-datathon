{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wrangle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import wrangle_COSA\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangle.wrangle_saws()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_df = wrangle.clean_air()\n",
    "air_df.datetime = pd.to_datetime(air_df.datetime)\n",
    "air_df = air_df.set_index('datetime')\n",
    "air_df = air_df.sort_index()\n",
    "air1_df = air_df[(air_df['Pm1_0'] < 500) & (air_df['Pm1_0'] > -50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(air_df.index, air_df.SO2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(air_df.index, air_df.O3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(air_df.index, air_df.CO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(air_df.index, air_df.NO2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = wrangle.wrangle_weather()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.dewpoint_celsius.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df['wind'] = weather_df['wind'].str.extract('(\\d+)', expand=False)\n",
    "weather_df['visibility'] = weather_df['visibility'].str.extract('(\\d+)', expand=False)\n",
    "weather_df['wind'] = weather_df['wind'].fillna(0)\n",
    "weather_df['wind'] = weather_df['wind'].apply(lambda x: int(x))\n",
    "weather_df['visibility'] = weather_df['visibility'].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_daily_COSA_dataframe():\n",
    "    sound_df = wrangle.wrangle_sound()\n",
    "    sound_df = sound_df.set_index('DateTime')\n",
    "    sound_df = sound_df.sort_index()\n",
    "    flood_df = wrangle.clean_flood()\n",
    "    flood_df = flood_df.set_index('datetime')\n",
    "    weather_df = wrangle.wrangle_weather()\n",
    "    air_df = wrangle.clean_air()\n",
    "    air_df.datetime = pd.to_datetime(air_df.datetime)\n",
    "    air_df = air_df.set_index('datetime')\n",
    "    air_df = air_df.sort_index()\n",
    "    weather_day_df = weather_df.resample('D', on='datetime').mean()\n",
    "    flood_day_df = flood_df.resample('D').mean()\n",
    "    sound_day_df = sound_df.resample('D').mean()\n",
    "    air_day_df = air_df.resample('D').mean().drop(columns = ['hour', 'weekday', 'CO_24hr', 'Pm_25_24hr', 'Pm_10_24hr', 'SO2', 'O3', 'NO2'])\n",
    "    air2_5 = air_df.drop(air_df.columns.difference(['Pm2_5', 'AQI_pm2_5']), 1)\n",
    "    air10 = air_df.drop(air_df.columns.difference(['Pm10', 'AQI_pm10']), 1)\n",
    "    airCO = air_df.drop(air_df.columns.difference(['CO', 'AQI_CO']), 1)\n",
    "    series2_5 = air2_5.resample('D').max().rename(columns = {'AQI_pm2_5': 'most_hazardous_pm2.5_level'})['most_hazardous_pm2.5_level']\n",
    "    series10 = air10.resample('D').max().rename(columns = {'AQI_pm10': 'most_hazardous_pm10_level'})['most_hazardous_pm10_level']\n",
    "    seriesCO = airCO.resample('D').max().rename(columns = {'AQI_CO': 'most_hazardous_CO_level'})['most_hazardous_CO_level']\n",
    "    hazards = pd.DataFrame(series2_5).join(series10).join(seriesCO)\n",
    "    df = weather_day_df.join(air_day_df).join(hazards).join(sound_day_df).join(flood_day_df)\n",
    "    df = df.round({'celsius': 2, 'farenheit': 2, 'humidity': 2, 'dewpoint_celsius': 2, 'dewpoint_farenheit': 2,\n",
    "          'pressure': 2, 'NoiseLevel_db': 2, 'sensor_to_water_feet': 2, 'sensor_to_water_meters': 2,\n",
    "          'sensor_to_ground_feet': 2, 'sensor_to_ground_meters': 2, 'flood_depth_feet': 2,\n",
    "          'flood_depth_meters': 2})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = wrangle.full_daily_COSA_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.Pm1_0.groupby(df.index.day_name()).mean().plot.bar(width=.9, ec='black')\n",
    "plt.xticks(rotation=0)\n",
    "ax.set(title='Average PM 1.0 Level by Day of Week', xlabel='Day of Week', ylabel='PM 1.0 Level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.Pm2_5.groupby(df.index.day_name()).mean().plot.bar(width=.9, ec='black')\n",
    "plt.xticks(rotation=0)\n",
    "ax.set(title='Average PM 2.5 Level by Day of Week', xlabel='Day of Week', ylabel='PM 2.5 Level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.Pm10.groupby(df.index.day_name()).mean().plot.bar(width=.9, ec='black')\n",
    "plt.xticks(rotation=0)\n",
    "ax.set(title='Average PM 10 Level by Day of Week', xlabel='Day of Week', ylabel='PM 10 Level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.CO.groupby(df.index.day_name()).mean().plot.bar(width=.9, ec='black')\n",
    "plt.xticks(rotation=0)\n",
    "ax.set(title='Average PM Carbon Monoxide Level by Day of Week', xlabel='Day of Week', ylabel='CO Level')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saws_df = wrangle.wrangle_saws()\n",
    "saws_df['year_month'] = '20' + saws_df['year_month']\n",
    "saws_df['year_month'] = pd.to_datetime(saws_df['year_month'])\n",
    "saws_month_year = saws_df.set_index('year_month').resample('M').mean().drop(columns = ['zipcode'])\n",
    "saws_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saws_month_year_sum = saws_df.set_index('year_month').resample('M').sum().drop(columns = ['zipcode'])\n",
    "saws_month_year_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_saws():\n",
    "    saws_df = wrangle.wrangle_saws()\n",
    "    saws_df['year_month'] = '20' + saws_df['year_month']\n",
    "    saws_df['year_month'] = pd.to_datetime(saws_df['year_month'])\n",
    "    saws_month_year = saws_df.set_index('year_month').resample('M').mean().drop(columns = ['zipcode'])\n",
    "    saws_month_year['Date'] = pd.to_datetime(saws_month_year.index)\n",
    "    saws_month_year['Mon_Year'] = saws_month_year['Date'].dt.strftime('%b-%Y')\n",
    "    saws_month_year_sum = saws_df.set_index('year_month').resample('M').sum().drop(columns = ['zipcode'])\n",
    "    saws_month_year_sum['Date'] = pd.to_datetime(saws_month_year_sum.index)\n",
    "    saws_month_year_sum['Mon_Year'] = saws_month_year_sum['Date'].dt.strftime('%b-%Y')\n",
    "    saws_places = saws_df.groupby('location').mean()\n",
    "    saws_places = saws_places.drop(columns =['zipcode'])\n",
    "    saws_places_sum = saws_df.groupby('location').sum()\n",
    "    import calendar\n",
    "    saws_year_month_mean = saws_df.gallons_consumed.groupby(saws_df['year_month'].dt.month).mean()\n",
    "    saws_year_month_mean = pd.DataFrame(saws_year_month_mean)\n",
    "    saws_year_month_mean['month'] = saws_year_month_mean.index\n",
    "    saws_year_month_mean['month'] = saws_year_month_mean['month'].apply(lambda x: calendar.month_abbr[x])\n",
    "    \n",
    "    plt.subplots(5, 1, figsize=(24, 40), sharey=True)\n",
    "    plt.subplots_adjust(hspace=.6)\n",
    "    sns.set(style=\"darkgrid\")\n",
    "        \n",
    "    plt.subplot(5, 1, 1)\n",
    "    plt.title('Average Monthly Water Use by Street')\n",
    "    plt.xticks(rotation = 90)\n",
    "    sns.barplot(data = saws_places, x = saws_places.index, y = 'gallons_consumed', palette = \"viridis\")\n",
    "    plt.xlabel('Street Name')\n",
    "    plt.ylabel('Average Monthly Gallons')\n",
    "    \n",
    "    plt.subplot(5, 1, 2)\n",
    "    plt.title('Sum of All Gallons Consumed by Street')\n",
    "    plt.xticks(rotation = 90)\n",
    "    sns.barplot(data = saws_places_sum, x = saws_places_sum.index, y = 'gallons_consumed', palette = \"viridis\")\n",
    "    plt.xlabel('Street Name')\n",
    "    plt.ylabel('Sum of Monthly Gallons')\n",
    "    \n",
    "    plt.subplot(5, 1, 3)\n",
    "    plt.title('Mean Property Consumption by Month Over 4 Year Period')\n",
    "    plt.xticks(rotation = 90)\n",
    "    sns.barplot(data = saws_month_year, x = saws_month_year['Mon_Year'], y = 'gallons_consumed', palette = \"viridis\")\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Mean Property Consumption')\n",
    "    \n",
    "    plt.subplot(5, 1, 4)\n",
    "    plt.title('Total Consumption by Month Over 4 Year Period in Medical Center')\n",
    "    plt.xticks(rotation = 90)\n",
    "    sns.barplot(data = saws_month_year_sum, x = saws_month_year_sum['Mon_Year'], y = 'gallons_consumed', palette = \"viridis\")\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Total Consumption')\n",
    "    \n",
    "    plt.subplot(5, 1, 5)\n",
    "    plt.title('Mean Property Consumption by Month of Year')\n",
    "    sns.barplot(data = saws_year_month_mean, x = 'month', y = 'gallons_consumed', palette = \"viridis\")\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Mean Property Consumption')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_saws()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sound_df = wrangle.wrangle_sound()\n",
    "sound_df = sound_df.set_index('DateTime')\n",
    "sound_df = sound_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sound_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot showing the average daily sound\n",
    "\n",
    "plt.subplots(figsize=(22, 6))\n",
    "plt.xticks(rotation = 90)\n",
    "sns.lineplot(data = sound_df.resample('D').mean(), x = sound_df.resample('D').mean().index, y = 'NoiseLevel_db', palette = \"magma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sound_df.index, sound_df.NoiseLevel_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_sound = sound_df.resample('H').mean()\n",
    "hourly_sound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of average sound levels by hour of the day\n",
    "\n",
    "hourly_sound = sound_df.resample('H').mean()\n",
    "hour_of_day_sound = hourly_sound.groupby(hourly_sound.index.hour).mean()\n",
    "plt.subplots(figsize=(22, 6))\n",
    "sns.barplot(data = hour_of_day_sound, x = hour_of_day_sound.index, y = 'NoiseLevel_db', palette = \"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sound_by_day = sound_df.groupby(sound_df.index.day_name()).mean()\n",
    "plt.subplots(figsize=(22, 6))\n",
    "sns.barplot(data = sound_by_day, x = sound_by_day.index, y = 'NoiseLevel_db', palette = \"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_df = wrangle.clean_flood()\n",
    "flood_df = flood_df.set_index('datetime')\n",
    "flood_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(22, 6))\n",
    "sns.lineplot(data = flood_df.resample('D').mean(), palette = \"magma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Air"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_df = wrangle.clean_air()\n",
    "air_df.datetime = pd.to_datetime(air_df.datetime)\n",
    "air_df = air_df.set_index('datetime')\n",
    "air_df = air_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air1_df = air_df[(air_df['Pm1_0'] < 500) & (air_df['Pm1_0'] > -50)]\n",
    "air1_day_df = air1_df.resample('D').mean()\n",
    "plt.subplots(figsize=(22, 6))\n",
    "sns.lineplot(data = air1_day_df, x = air1_day_df.index, y = 'Pm1_0', palette = \"magma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.subplots(figsize=(22, 6))\n",
    "sns.barplot(data = air_by_day, x = air_by_day.index, y = 'Pm1_0', palette = \"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(22, 6))\n",
    "sns.barplot(data = air_by_day, x = sound_by_day.index, y = 'Pm2_5', palette = \"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(22, 6))\n",
    "sns.barplot(data = air_by_day, x = sound_by_day.index, y = 'Pm10', palette = \"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(22, 6))\n",
    "sns.barplot(data = air_by_day, x = sound_by_day.index, y = 'CO', palette = \"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_air():\n",
    "    air_df = wrangle.clean_air()\n",
    "    air_df.datetime = pd.to_datetime(air_df.datetime)\n",
    "    air_df = air_df.set_index('datetime')\n",
    "    air_df = air_df.sort_index()\n",
    "    air1_df = air_df[(air_df['Pm1_0'] < 500) & (air_df['Pm1_0'] > -50)]\n",
    "    air1_day_df = air1_df.resample('D').mean()\n",
    "    air_df = air_df[(air_df['Pm1_0'] < 500) & (air_df['Pm1_0'] > -50)]\n",
    "    air_df = air_df[(air_df['Pm2_5'] < 500) & (air_df['Pm2_5'] > -50)]\n",
    "    air_df = air_df[(air_df['Pm10'] < 500) & (air_df['Pm10'] > -50)]\n",
    "    air_by_day = air_df.groupby(air_df.index.day_name()).mean()\n",
    "\n",
    "    plt.subplots(5, 1, figsize=(24, 40), sharey=True)\n",
    "    plt.subplots_adjust(hspace=.6)\n",
    "    sns.set(style=\"darkgrid\")\n",
    "    \n",
    "    plt.subplot(5, 1, 1)\n",
    "    plt.title('Daily Mean PM 1.0 Levels')\n",
    "    sns.lineplot(data = air1_day_df, x = air1_day_df.index, y = 'Pm1_0')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Reading')\n",
    "    \n",
    "    plt.subplot(5, 1, 2)\n",
    "    plt.title('Mean PM 1.0 Levels by Day of Week')\n",
    "    sns.barplot(data = air_by_day, x = air_by_day.index, y = 'Pm1_0', palette = \"magma\")\n",
    "    plt.xlabel('Day')\n",
    "    plt.ylabel('Reading')\n",
    "    \n",
    "    plt.subplot(5, 1, 3)\n",
    "    plt.title('Mean PM 2.5 Levels by Day of Week')\n",
    "    sns.barplot(data = air_by_day, x = air_by_day.index, y = 'Pm2_5', palette = \"magma\")\n",
    "    plt.xlabel('Day')\n",
    "    plt.ylabel('Reading')\n",
    "    \n",
    "    plt.subplot(5, 1, 4)\n",
    "    plt.title('Mean PM 10 Levels by Day of Week')\n",
    "    sns.barplot(data = air_by_day, x = air_by_day.index, y = 'Pm10', palette = \"magma\")\n",
    "    plt.xlabel('Day')\n",
    "    plt.ylabel('Reading')\n",
    "    \n",
    "    plt.subplot(5, 1, 5)\n",
    "    plt.title('Mean Carbon Monoxide Levels by Day of Week')\n",
    "    sns.barplot(data = air_by_day, x = air_by_day.index, y = 'CO', palette = \"magma\")\n",
    "    plt.xlabel('Day')\n",
    "    plt.ylabel('Reading')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_air()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('jan_2018.csv')['b.long'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('jan_2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_flood(flood):\n",
    "    '''Drops unneeded columns from the med center flooding df\n",
    "    Makes sure DateTime is in DateTime format'''\n",
    "    # drop the columns\n",
    "    flood = flood.drop(columns=['LAT', 'LONG', 'Zone',  \n",
    "                          'SensorStatus', 'AlertTriggered', \n",
    "                          'Temp_C', 'Temp_F', 'Vendor'])\n",
    "    # Set to date time format\n",
    "    flood.DateTime = pd.to_datetime(flood.DateTime)\n",
    "    flood = flood.rename(columns={\"DateTime\": \"datetime\", \n",
    "                        \"DistToWL_ft\": \"sensor_to_water_feet\", \n",
    "                        \"DistToWL_m\": \"sensor_to_water_meters\", \n",
    "                        \"DistToDF_ft\": \"sensor_to_ground_feet\",\n",
    "                        \"DistToDF_m\": \"sensor_to_ground_meters\"})\n",
    "    # replae -999 with 0\n",
    "    flood[\"sensor_to_ground_feet\"].replace({-999:13.5006561680}, inplace=True)\n",
    "    flood[\"sensor_to_ground_meters\"].replace({-999:4.115}, inplace=True)\n",
    "    \n",
    "    #flood = flood.replace(to_replace=-999, value=0)\n",
    "    # create new features for flood depth\n",
    "    flood['flood_depth_feet'] = flood.sensor_to_ground_feet - flood.sensor_to_water_feet\n",
    "    flood['flood_depth_meters'] = flood.sensor_to_ground_meters - flood.sensor_to_water_meters \n",
    "    # Create new alert\n",
    "    def flood_alert(c):\n",
    "        if 0 < c['flood_depth_feet'] < 0.66667:\n",
    "            return 'No Risk'\n",
    "        elif 0.66667 < c['flood_depth_feet'] < 1.08333:\n",
    "            return 'Minor Risk'\n",
    "        elif 1.08333 < c['flood_depth_feet'] < 2.16667:\n",
    "            return 'Moderate Risk'\n",
    "        elif 2.16667 < c['flood_depth_feet']:\n",
    "            return 'Major Risk !'\n",
    "        else:\n",
    "            return 'No Alert'\n",
    "    flood['flood_alert'] = flood.apply(flood_alert, axis=1)\n",
    "    flood = flood[(flood.sensor_to_water_feet != -999)]\n",
    "    # return new df\n",
    "    return flood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_air(air):\n",
    "    '''Drops unneeded columns from the air quality df\n",
    "    then handles the nulls in alert triggered column\n",
    "    set to date time format'''\n",
    "    # drop the colums\n",
    "    air = air.drop(columns=['LAT', 'LONG', 'Zone', \n",
    "                            'Sensor_id', 'SensorModel', \n",
    "                            'SensorStatus', 'Vendor'])\n",
    "    # replace nulls in ALertTriggered to None\n",
    "    air.fillna(\"None\", inplace = True)\n",
    "    # set to date time format\n",
    "    air.DateTime = pd.to_datetime(air.DateTime)\n",
    "    # rename features\n",
    "    air = air.rename(columns={\"DateTime\": \"datetime\",\n",
    "                              \"AlertTriggered\":\"alert_triggered\"})\n",
    "    air = air.replace(to_replace=-999, value=0)\n",
    "    # create time series features\n",
    "    air['dates'] = pd.to_datetime(air['datetime']).dt.date\n",
    "    air['time'] = pd.to_datetime(air['datetime']).dt.time\n",
    "    air['hour'] = pd.to_datetime(air['datetime']).dt.hour\n",
    "    air['weekday'] = pd.to_datetime(air['datetime']).dt.weekday\n",
    "    # make all CO bins\n",
    "    air['AQI_CO'] = pd.cut(air.CO, \n",
    "                            bins = [-1,4.5,9.5,12.5,15.5,30.5,4000],\n",
    "                            labels = ['Good', 'Moderate', \n",
    "                                      'Unhealthy for Sensitive Groups', \"Unhealthy\", \n",
    "                                      \"Very Unhealthy\", 'Hazardous'])\n",
    "    \n",
    "    CO_24hr = air.groupby('dates', as_index=False)['CO'].mean()\n",
    "    CO_24hr = CO_24hr.rename(columns={'CO':'CO_24hr'})\n",
    "    air = air.merge(CO_24hr, on = 'dates', how ='left')\n",
    "    air['AQI_CO_24hr'] = pd.cut(air.CO_24hr, \n",
    "                                bins = [-1,4.5,9.5,12.5,15.5,30.5,4000],\n",
    "                                labels = ['Good', 'Moderate', \n",
    "                                          'Unhealthy for Sensitive Groups', \"Unhealthy\", \n",
    "                                          \"Very Unhealthy\", 'Hazardous'])\n",
    "    \n",
    "    air['AQI_pm2_5'] = pd.cut(air.Pm2_5, \n",
    "                                bins = [-1,12.1,35.5,55.5,150.5,250.5,4000],\n",
    "                                labels = ['Good', 'Moderate', \n",
    "                                          'Unhealthy for Sensitive Groups', \"Unhealthy\", \n",
    "                                          \"Very Unhealthy\", 'Hazardous'])\n",
    "    pm_25_24hr = air.groupby('dates', as_index=False)['Pm2_5'].mean()\n",
    "    pm_25_24hr = pm_25_24hr.rename(columns={'Pm2_5':'Pm_25_24hr'})\n",
    "    air = air.merge(pm_25_24hr, on = 'dates', how ='left')\n",
    "    air['AQI_pm_25_24hr'] = pd.cut(air.Pm_25_24hr, \n",
    "                                bins = [-1,12.1,35.5,55.5,150.5,250.5,4000],\n",
    "                                labels = ['Good', 'Moderate', \n",
    "                                          'Unhealthy for Sensitive Groups', \"Unhealthy\", \n",
    "                                          \"Very Unhealthy\", 'Hazardous'])\n",
    "    \n",
    "    air['AQI_pm10'] = pd.cut(air.Pm10, \n",
    "                                bins = [-1,55,154,255,355,425,4000],\n",
    "                                labels = ['Good', 'Moderate', \n",
    "                                          'Unhealthy for Sensitive Groups', \"Unhealthy\", \n",
    "                                          \"Very Unhealthy\", 'Hazardous'])\n",
    "    pm_10_24hr = air.groupby('dates', as_index=False)['Pm10'].mean()\n",
    "    pm_10_24hr = pm_10_24hr.rename(columns={'Pm10':'Pm_10_24hr'})\n",
    "    air = air.merge(pm_10_24hr, on = 'dates', how ='left')\n",
    "    air['AQI_pm10_24hr'] = pd.cut(air.Pm_10_24hr, \n",
    "                                bins = [-1,55,154,255,355,425,4000],\n",
    "                                labels = ['Good', 'Moderate', \n",
    "                                          'Unhealthy for Sensitive Groups', \"Unhealthy\", \n",
    "                                          \"Very Unhealthy\", 'Hazardous'])\n",
    "    return air"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle_weather(weather):\n",
    "    '''\n",
    "    This function will drop unneccessary columns, \n",
    "    change datetime to a pandas datetime datatype,\n",
    "    and rename columns to be more readable to return\n",
    "    a clean dataframe.  \n",
    "    '''\n",
    "    #read csv and turn into pandas dataframe\n",
    "    sa_weather = pd.read_csv('SA_weather.csv')\n",
    "    # concat sa date and time\n",
    "    sa_weather['Date_Time'] = sa_weather['Date'] + ' ' + sa_weather['Time']\n",
    "    # put into date time format\n",
    "    sa_weather.Date_Time = pd.to_datetime(sa_weather.Date_Time)\n",
    "    # round to nearest hour\n",
    "    sa_weather['DateTime'] = sa_weather['Date_Time'].dt.round('60min')\n",
    "    # set sa weather index\n",
    "    sa_weather = sa_weather.set_index('DateTime')\n",
    "    # drop old datetime\n",
    "    sa_weather = sa_weather.drop(columns=['Date_Time', 'Temp', 'Humidity', 'Barometer'])\n",
    "    # rename\n",
    "    sa_weather = sa_weather.rename(columns={\"Time\": \"time\", \n",
    "                            \"Date\": \"date\", \n",
    "                            \"Weather\": \"weather\", \n",
    "                            \"Wind\": \"wind\",\n",
    "                            \"Visibility\": \"visibility\"})\n",
    "    #drop columns we will not be using\n",
    "    weather.drop(columns=[\n",
    "    'Sensor_id', \n",
    "    'Vendor', \n",
    "    'SensorModel', \n",
    "    'LAT', \n",
    "    'LONG', \n",
    "    'Zone', \n",
    "    'AlertTriggered', \n",
    "    'SensorStatus'], inplace=True)\n",
    "    #rename columns to be more readable\n",
    "    weather = weather.rename(columns={\"DateTime\": \"datetime\", \n",
    "                            \"Temp_C\": \"celsius\", \n",
    "                            \"Temp_F\": \"farenheit\", \n",
    "                            \"Humidity\": \"humidity\",\n",
    "                            \"DewPoint_C\": \"dewpoint_celsius\",\n",
    "                            \"DewPoint_F\": \"dewpoint_farenheit\",\n",
    "                            \"Pressure_Pa\": \"pressure\"})\n",
    "    #change datetime to pandas datetime object\n",
    "    weather.datetime = pd.to_datetime(weather.datetime)\n",
    "    # round to hour\n",
    "    weather['DateTime'] = weather['datetime'].dt.round('60min')\n",
    "    # set index\n",
    "    weather = weather.set_index('DateTime')\n",
    "    # join the 2 df's\n",
    "    weather = weather.join(sa_weather, how='right')\n",
    "    # repalce -999\n",
    "    weather = weather.replace(to_replace=-999, value=0)\n",
    "    # drop nulls\n",
    "    weather.dropna(inplace = True)\n",
    "    #return clean weather df\n",
    "    return weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle_sound(df):\n",
    "    '''\n",
    "    This function drops unnecessary columns and\n",
    "    converts the 'DateTime' column to a datetime \n",
    "    object\n",
    "    '''\n",
    "\n",
    "    # Drops unnecessary columns\n",
    "    df = df.drop(columns = ['SensorStatus', 'AlertTriggered', 'Zone', 'LONG', \n",
    "                            'LAT', 'SensorModel', 'Vendor', 'Sensor_id'])\n",
    "    # Converts to datetime\n",
    "    df['DateTime'] = pd.to_datetime(df.DateTime)\n",
    "    # make noise level feature\n",
    "    df['how_loud'] = pd.cut(df.NoiseLevel_db, \n",
    "                                bins = [-1,46,66,81,101,4000],\n",
    "                                labels = ['Normal', 'Moderate', \n",
    "                                          'Loud', \"Very Loud\", \n",
    "                                          \"Extremely Loud\"])\n",
    "    def sound_alert(c):\n",
    "        if c['NoiseLevel_db'] > 80:\n",
    "            return 'Minor Risk'\n",
    "        elif c['NoiseLevel_db'] > 120:\n",
    "            return 'Major Risk'\n",
    "        else:\n",
    "            return 'No Alert'\n",
    "    df['sound_alert'] = df.apply(sound_alert, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_daily_downtown_COSA_dataframe():\n",
    "    \n",
    "    '''\n",
    "    This function takes in all COSA dataframes,\n",
    "    averages them by day, then joins them all together\n",
    "    using the day as a primary key\n",
    "    '''\n",
    "\n",
    "    # Pulls sound CSV and sets datetime as index, then orders it\n",
    "    df = pd.read_csv('downtown_sound.csv')\n",
    "    sound_df = wrangle_sound(df)\n",
    "    sound_df = sound_df.set_index('DateTime')\n",
    "    sound_df = sound_df.sort_index()\n",
    "    # Pulls flood CSV and sets datetime as index\n",
    "    flood = pd.read_csv('downtown_flood.csv')\n",
    "    flood_df = clean_flood(flood)\n",
    "    flood_df = flood_df.set_index('datetime')\n",
    "    # Pulls weather CSV\n",
    "    weather = pd.read_csv('downtown_weather.csv')\n",
    "    weather_df = wrangle_weather(weather)\n",
    "    # Pulls air CSV, sets datetime column to datetime object, sets it as an index, then sorts it\n",
    "    air = pd.read_csv('downtown_air.csv')\n",
    "    air_df = clean_air(air)\n",
    "    air_df.datetime = pd.to_datetime(air_df.datetime)\n",
    "    air_df = air_df.set_index('datetime')\n",
    "    air_df = air_df.sort_index()\n",
    "    # Resamples each dataframe by the day using mean, and drops unnecessary columns from air_df\n",
    "    weather_day_df = weather_df.resample('D', on='datetime').mean()\n",
    "    flood_day_df = flood_df.resample('D').mean()\n",
    "    sound_day_df = sound_df.resample('D').mean()\n",
    "    air_day_df = air_df.resample('D').mean().drop(columns = ['hour', 'weekday', 'CO_24hr', 'Pm_25_24hr', 'Pm_10_24hr', 'SO2', 'O3', 'NO2'])\n",
    "    # Creating series for each pollutant\n",
    "    air2_5 = air_df.drop(air_df.columns.difference(['Pm2_5', 'AQI_pm2_5']), 1)\n",
    "    air10 = air_df.drop(air_df.columns.difference(['Pm10', 'AQI_pm10']), 1)\n",
    "    airCO = air_df.drop(air_df.columns.difference(['CO', 'AQI_CO']), 1)\n",
    "    # Pull most hazardous levels of pollution for each day\n",
    "    series2_5 = air2_5.resample('D').max().rename(columns = {'AQI_pm2_5': 'most_hazardous_pm2.5_level'})['most_hazardous_pm2.5_level']\n",
    "    series10 = air10.resample('D').max().rename(columns = {'AQI_pm10': 'most_hazardous_pm10_level'})['most_hazardous_pm10_level']\n",
    "    seriesCO = airCO.resample('D').max().rename(columns = {'AQI_CO': 'most_hazardous_CO_level'})['most_hazardous_CO_level']\n",
    "    # Joins the series together in a dataframe\n",
    "    hazards = pd.DataFrame(series2_5).join(series10).join(seriesCO)\n",
    "    # Joins the resampled dataframes together\n",
    "    df = weather_day_df.join(air_day_df).join(hazards).join(sound_day_df).join(flood_day_df)\n",
    "    # Rounds numbers in specific columns\n",
    "    df = df.round({'celsius': 2, 'farenheit': 2, 'humidity': 2, 'dewpoint_celsius': 2, 'dewpoint_farenheit': 2,\n",
    "          'pressure': 2, 'NoiseLevel_db': 2, 'sensor_to_water_feet': 2, 'sensor_to_water_meters': 2,\n",
    "          'sensor_to_ground_feet': 2, 'sensor_to_ground_meters': 2, 'flood_depth_feet': 2,\n",
    "          'flood_depth_meters': 2})\n",
    "    # Create AQI for CO\n",
    "    df['AQI_CO'] = pd.cut(df.CO, \n",
    "                            bins = [-1,4.5,9.5,12.5,15.5,30.5,4000],\n",
    "                            labels = ['Good', 'Moderate', \n",
    "                                      'Unhealthy for Sensitive Groups', \"Unhealthy\", \n",
    "                                      \"Very Unhealthy\", 'Hazardous'])\n",
    "    # create AQi for pm 2.5\n",
    "    df['AQI_pm2_5'] = pd.cut(df.Pm2_5, \n",
    "                                bins = [-1,12.1,35.5,55.5,150.5,250.5,4000],\n",
    "                                labels = ['Good', 'Moderate', \n",
    "                                          'Unhealthy for Sensitive Groups', \"Unhealthy\", \n",
    "                                          \"Very Unhealthy\", 'Hazardous'])\n",
    "    # create AQI for pm 10\n",
    "    df['AQI_pm10'] = pd.cut(df.Pm10, \n",
    "                                bins = [-1,55,154,255,355,425,4000],\n",
    "                                labels = ['Good', 'Moderate', \n",
    "                                          'Unhealthy for Sensitive Groups', \"Unhealthy\", \n",
    "                                          \"Very Unhealthy\", 'Hazardous'])\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = full_daily_downtown_COSA_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangle_COSA.full_daily_downtown_COSA_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangle_COSA.full_daily_brooks_COSA_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangle_COSA.full_daily_medcenter_COSA_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulls the data\n",
    "saws_df = wrangle.wrangle_saws()\n",
    "# Creates a new column that has the date in a \"year month\" format that can be converted to a datetime object\n",
    "saws_df['year_month'] = '20' + saws_df['year_month']\n",
    "# Converts to datetime\n",
    "saws_df['year_month'] = pd.to_datetime(saws_df['year_month'])\n",
    "# Sets the index to the datetime object, then groups the data by month and drops zipcode (since they are all in the same one)\n",
    "saws_month_year = saws_df.set_index('year_month').resample('M').mean().drop(columns = ['zipcode'])\n",
    "# Creates a new column for labeling graphs with month and year\n",
    "saws_month_year['Date'] = pd.to_datetime(saws_month_year.index)\n",
    "# Truncates the 'Date' column into just month and year\n",
    "saws_month_year['Mon_Year'] = saws_month_year['Date'].dt.strftime('%b-%Y')\n",
    "# Creates a new dataframe for the mean of gallons used by street, drops zipcode column\n",
    "saws_places = saws_df.groupby('location').mean().drop(columns =['zipcode']).reset_index()\n",
    "# Creates a new dataframe for the sum of gallons used by street\n",
    "#saws_places_sum = saws_df.groupby('location').sum()\n",
    "#import calendar\n",
    "#saws_df = saws_df.replace('*', 0)\n",
    "#saws_df['gallons_consumed'] = pd.to_numeric(saws_df['gallons_consumed'])\n",
    "## Creates dataframe for repesenting months by mean monthly water usage\n",
    "#saws_year_month_mean = saws_df.gallons_consumed.groupby(saws_df['year_month'].dt.month).mean()\n",
    "#saws_year_month_mean = pd.DataFrame(saws_year_month_mean)\n",
    "#saws_year_month_mean['month'] = saws_year_month_mean.index\n",
    "## Labels numeric months into their respective shortened titles\n",
    "#saws_year_month_mean['month'] = saws_year_month_mean['month'].apply(lambda x: calendar.month_abbr[x])\n",
    "#plt.subplots(4, 1, figsize=(24, 40), sharey=True)\n",
    "#plt.subplots_adjust(hspace=.5)\n",
    "#sns.set(style=\"darkgrid\")\n",
    "#    \n",
    "#plt.subplot(4, 1, 1)\n",
    "#plt.title('Average Monthly Water Use by Street')\n",
    "#plt.xticks(rotation = 90)\n",
    "#sns.barplot(data = saws_places, x = 'location', y = 'gallons_consumed', palette = \"viridis\")\n",
    "#plt.xlabel('Street Name')\n",
    "#plt.ylabel('Average Monthly Gallons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saws_places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saws_df.groupby('location').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangle.show_saws()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
